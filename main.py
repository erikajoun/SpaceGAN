from __future__ import print_function
import os
import torch
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
import torchvision
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.autograd import Variable

from generator import Generator
from discriminator import Discriminator

SAVE_DIR = './models/'
SAVE_FILE = 'checkpoint.pth.tar'
SAVE_PATH = SAVE_DIR + SAVE_FILE
SAVE_INTERVAL = 10
NUM_EPOCHS = 10000

batchSize = 64 # Size of the batch
imageSize = 64 # Size of the generated images

# Will apply scaling, tensor conversion, and normalization to the input images
transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])

# Function to save checkpoint for resuming training from epoch
def save_checkpoint(state, filename = SAVE_DIR + SAVE_FILE):
    print('Saving checkpoint')
    torch.save(state, filename)
    print('Saved checkpoint')

# Load directory containing images as a training dataset
def load_dataset():
    data_path = './data/'
    train_dataset = torchvision.datasets.ImageFolder(
        root=data_path,
        transform=transform
    )
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batchSize,
        num_workers=2,
        shuffle=True
    )
    return train_loader

dataloader = load_dataset();

# Function to nitialize all the weights of a neural network
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        m.weight.data.normal_(0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)

# Creating the generator and discriminator
netG = Generator()
netD = Discriminator()

netG.apply(weights_init)
netD.apply(weights_init)

criterion = nn.BCELoss() # Criterion object to measure the loss

# Optimizers for the generator and discriminator
optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))
optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))

start_epoch = 0

# Load checkpoint if saved model is found
if os.path.isfile(SAVE_PATH):
    print("Loading checkpoint")
    checkpoint = torch.load(SAVE_PATH)
    start_epoch = checkpoint['epoch']
    netG.load_state_dict(checkpoint['netG_state_dict'])
    netD.load_state_dict(checkpoint['netD_state_dict'])
    optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])
    optimizerD.load_state_dict(checkpoint['optimizerD_state_dict'])
else:
    print("No checkpoint found")

# Training the DCGAN
for epoch in range(start_epoch, NUM_EPOCHS):
    # Iterate over each image in the dataset
    for i, data in enumerate(dataloader, 0):
        # Train the discriminator with a real image from the dataset
        netD.zero_grad() #  Initialize to 0 the gradients of the discriminator with respect to the weights
        real, _ = data # Get real image from the dataset
        input = Variable(real)
        output = netD(input) # Output is a value between 0 and 1 for the probability that the input is a real image
        target = Variable(torch.ones(input.size()[0]))
        errD_real = criterion(output, target) # Compute the loss between output and 1 for a real image
        
        # Train the discriminator with a fake image generated by the generator
        noise = Variable(torch.randn(input.size()[0], 100, 1, 1)) # Pass in random input vector for the generator as noise
        fake = netG(noise) # Get fake generated images
        output = netD(fake.detach()) # Output is a value between 0 and 1 for the probability that the input is a real image
        target = Variable(torch.zeros(input.size()[0])) 
        errD_fake = criterion(output, target) # Compute the loss between output and 0 for a fake image

        # Backpropagate the total error for the discriminator and update weights
        errD = errD_real + errD_fake
        errD.backward() # Compute the gradients of the total error with respect to the weights of the discriminator
        optimizerD.step() # Update the weights according to how much they are responsible for the loss of the discriminator

        # Train the generator by aiming for an output of 1 from the discriminator
        netG.zero_grad()
        output = netD(fake) # Output is a value between 0 and 1 for the probability that the input is a real image
        target = Variable(torch.ones(input.size()[0])) # Compute the loss between output and 1 for the fake generated image
        errG = criterion(output, target)
        errG.backward() # Compute the gradients of the total error with respect to the weights of the generator
        optimizerG.step() # Update the weights according to how much they are responsible for the loss of the generator

        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, NUM_EPOCHS, i, len(dataloader), errD.data[0], errG.data[0])) # We print les losses of the discriminator (Loss_D) and the generator (Loss_G).
    
    # Save images every epoch
    vutils.save_image(real, '%s/real_samples.png' % "./results", normalize = True) # Save real images of the batch
    fake = netG(noise) # Save fake generated images of the batch
    vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % ("./results", epoch), normalize = True) # Save the fake generated images of the minibatch
        
    # Save models for resuming training every set epochs
    if epoch != 0 and epoch % SAVE_INTERVAL == 0:
        save_checkpoint({
            'epoch': epoch + 1,
            'netG_state_dict': netG.state_dict(),
            'netD_state_dict': netD.state_dict(),
            'optimizerG_state_dict': optimizerG.state_dict(),
            'optimizerD_state_dict': optimizerD.state_dict()
        })